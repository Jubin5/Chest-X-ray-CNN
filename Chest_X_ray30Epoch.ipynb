{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rMBchMvYAx89"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import  ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Concatenate,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/chest_xray.zip\"\n",
        "extract_path = \"/content/chest_xray_data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ],
      "metadata": {
        "id": "sKHc7VsVA8o9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for folder in os.listdir(extract_path):\n",
        "    print(\"📂\", folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DnfeFeDBDJ-",
        "outputId": "b610c10a-374c-4962-f86c-5257e0cfb2df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 test\n",
            "📂 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/chest_xray_data\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Image Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # we'll split here\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper to wrap generator output with proper structure\n",
        "def duplicate_input(generator):\n",
        "    for x, y in generator:\n",
        "        yield (x, x), y  # Create 2 inputs for ensemble\n",
        "\n",
        "\n",
        "# Define output_signature here\n",
        "output_signature = (\n",
        "    (\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
        "    ),\n",
        "    tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create dual input test set\n",
        "test_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(test_gen),\n",
        "    output_signature=(\n",
        "        (\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
        "        ),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "# Wrap the generator with tf.data.Dataset\n",
        "train_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(train_gen),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "val_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(val_gen),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Data Augmentation only on training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.15  # you can adjust this if needed\n",
        ")\n",
        "\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/chest_xray_data/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Base models\n",
        "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom heads\n",
        "vgg_out = GlobalAveragePooling2D()(vgg.output)\n",
        "resnet_out = GlobalAveragePooling2D()(resnet.output)\n",
        "\n",
        "\n",
        "combined = Concatenate()([vgg_out, resnet_out])\n",
        "combined = Dropout(0.5)(combined)  # Dropout added here after concatenation\n",
        "combined = Dense(256, activation='relu')(combined)\n",
        "combined = Dropout(0.5)(combined)  # Dropout after Dense layer\n",
        "output = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "\n",
        "model = Model(inputs=[vgg.input, resnet.input], outputs=output)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "# Note: We'll use two separate generators for VGG and ResNet if needed, but this model assumes same input\n",
        "model.fit(\n",
        "    train_dual_input,\n",
        "    validation_data=val_dual_input,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    epochs=30\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "model.evaluate(test_dual_input, steps=len(test_gen))\n",
        "\n",
        "# Save model\n",
        "# model.save('ensemble_cxr_model.keras')\n",
        "import joblib\n",
        "\n",
        "# Assume model is a scikit-learn model or any picklable object\n",
        "joblib.dump(model, \"ensemble_cxr_model.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPy2ynBBBEfT",
        "outputId": "67c466b5-18e3-49c3-996e-c64095371220"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4187 images belonging to 2 classes.\n",
            "Found 1045 images belonging to 2 classes.\n",
            "Found 4448 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 924ms/step - accuracy: 0.6864 - loss: 0.6828 - val_accuracy: 0.7426 - val_loss: 0.5013\n",
            "Epoch 2/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 807ms/step - accuracy: 0.7202 - loss: 0.5837 - val_accuracy: 0.7703 - val_loss: 0.4542\n",
            "Epoch 3/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 779ms/step - accuracy: 0.7478 - loss: 0.5195 - val_accuracy: 0.8383 - val_loss: 0.4116\n",
            "Epoch 4/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.7666 - loss: 0.4820 - val_accuracy: 0.8727 - val_loss: 0.3895\n",
            "Epoch 5/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.7930 - loss: 0.4383 - val_accuracy: 0.8938 - val_loss: 0.3568\n",
            "Epoch 6/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8037 - loss: 0.4164 - val_accuracy: 0.8861 - val_loss: 0.3668\n",
            "Epoch 7/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 808ms/step - accuracy: 0.8027 - loss: 0.3981 - val_accuracy: 0.8890 - val_loss: 0.3572\n",
            "Epoch 8/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 791ms/step - accuracy: 0.8121 - loss: 0.3793 - val_accuracy: 0.8871 - val_loss: 0.3194\n",
            "Epoch 9/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8272 - loss: 0.3826 - val_accuracy: 0.8833 - val_loss: 0.3297\n",
            "Epoch 10/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8311 - loss: 0.3658 - val_accuracy: 0.8938 - val_loss: 0.3064\n",
            "Epoch 11/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8430 - loss: 0.3560 - val_accuracy: 0.8976 - val_loss: 0.3076\n",
            "Epoch 12/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 790ms/step - accuracy: 0.8485 - loss: 0.3316 - val_accuracy: 0.8967 - val_loss: 0.3042\n",
            "Epoch 13/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 789ms/step - accuracy: 0.8513 - loss: 0.3359 - val_accuracy: 0.8995 - val_loss: 0.2873\n",
            "Epoch 14/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 803ms/step - accuracy: 0.8383 - loss: 0.3434 - val_accuracy: 0.9072 - val_loss: 0.2648\n",
            "Epoch 15/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8556 - loss: 0.3285 - val_accuracy: 0.8890 - val_loss: 0.2956\n",
            "Epoch 16/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8549 - loss: 0.3172 - val_accuracy: 0.9005 - val_loss: 0.2818\n",
            "Epoch 17/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 782ms/step - accuracy: 0.8533 - loss: 0.3212 - val_accuracy: 0.9043 - val_loss: 0.2734\n",
            "Epoch 18/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 795ms/step - accuracy: 0.8681 - loss: 0.3070 - val_accuracy: 0.9072 - val_loss: 0.2645\n",
            "Epoch 19/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8591 - loss: 0.3129 - val_accuracy: 0.9024 - val_loss: 0.2630\n",
            "Epoch 20/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8662 - loss: 0.3137 - val_accuracy: 0.9081 - val_loss: 0.2642\n",
            "Epoch 21/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 802ms/step - accuracy: 0.8550 - loss: 0.3107 - val_accuracy: 0.9062 - val_loss: 0.2617\n",
            "Epoch 22/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 786ms/step - accuracy: 0.8706 - loss: 0.2976 - val_accuracy: 0.9043 - val_loss: 0.2479\n",
            "Epoch 23/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 821ms/step - accuracy: 0.8791 - loss: 0.2843 - val_accuracy: 0.9100 - val_loss: 0.2566\n",
            "Epoch 24/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 794ms/step - accuracy: 0.8793 - loss: 0.2854 - val_accuracy: 0.9081 - val_loss: 0.2634\n",
            "Epoch 25/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8815 - loss: 0.2806 - val_accuracy: 0.8823 - val_loss: 0.3022\n",
            "Epoch 26/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 784ms/step - accuracy: 0.8780 - loss: 0.2802 - val_accuracy: 0.9120 - val_loss: 0.2340\n",
            "Epoch 27/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8748 - loss: 0.2950 - val_accuracy: 0.9100 - val_loss: 0.2353\n",
            "Epoch 28/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 789ms/step - accuracy: 0.8753 - loss: 0.2825 - val_accuracy: 0.9024 - val_loss: 0.2645\n",
            "Epoch 29/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8854 - loss: 0.2737 - val_accuracy: 0.9167 - val_loss: 0.2445\n",
            "Epoch 30/30\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 798ms/step - accuracy: 0.8749 - loss: 0.2820 - val_accuracy: 0.9100 - val_loss: 0.2392\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8380 - loss: 0.3713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ensemble_cxr_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to change the .pkl to .keras file\n",
        "import joblib\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the model from the .pkl file\n",
        "loaded_model = joblib.load(\"/content/ensemble_cxr_model (1).pkl\")\n",
        "\n",
        "# Save the model as a .keras file\n",
        "loaded_model.save('ensemble_cxr_new_model.keras')\n"
      ],
      "metadata": {
        "id": "YYbu8sJ6UOtX"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}